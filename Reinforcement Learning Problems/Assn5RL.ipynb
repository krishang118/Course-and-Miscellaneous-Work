{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmFjc2wcACg1",
        "outputId": "bc981357-b8c9-4597-9889-e569a36f6e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The MDP Problem Solution:\n",
            "\n",
            "Discount Factor (γ): 0.1\n",
            "Long-Term Discounted Rewards:\n",
            "State S0: 4.4777\n",
            "State S1: 7.5449\n",
            "State S2: 1.3854\n",
            "Convergence Iterations: 10\n",
            "\n",
            "Discount Factor (γ): 0.01\n",
            "Long-Term Discounted Rewards:\n",
            "State S0: 4.0434\n",
            "State S1: 7.0495\n",
            "State S2: 1.0344\n",
            "Convergence Iterations: 6\n",
            "\n",
            "Discount Factor (γ): 0.001\n",
            "Long-Term Discounted Rewards:\n",
            "State S0: 4.0043\n",
            "State S1: 7.0049\n",
            "State S2: 1.0034\n",
            "Convergence Iterations: 4\n",
            "\n",
            "Discount Factor (γ): 0.3\n",
            "Long-Term Discounted Rewards:\n",
            "State S0: 5.8501\n",
            "State S1: 9.1134\n",
            "State S2: 2.5555\n",
            "Convergence Iterations: 19\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "class mdpprog:\n",
        "    def __init__(self):\n",
        "        self.rewards = {'S0': 4,'S1': 7,'S2': 1}\n",
        "        self.transition_matrices = {\n",
        "            #random values assumed for a0 matrix (as not given in the question)\n",
        "            'a0': np.array([\n",
        "                [0.25, 0.6, 0.4],\n",
        "                [0.45, 0.4, 0.30],\n",
        "                [0.25, 0.3, 0.5]]),\n",
        "            'a1': np.array([\n",
        "                [0.3, 0.4, 0.3],\n",
        "                [0.6, 0.2, 0.2],\n",
        "                [0.2, 0.2, 0.6]]),\n",
        "            'a2': np.array([\n",
        "                [0.5, 0.4, 0.1],\n",
        "                [0.3, 0.2, 0.5],\n",
        "                [0.4, 0.2, 0.4]]),\n",
        "            'a3': np.array([\n",
        "                [0.25, 0.25, 0.5],\n",
        "                [0.4, 0.3, 0.3],\n",
        "                [0.2, 0.5, 0.3]])}\n",
        "        self.policyfunc = {\n",
        "            'S0': 'a1','S1': 'a0','S2': 'a2'}\n",
        "    def calclongtermreward(self, discountfactor: float, maxiter: int = 5000) -> tuple[npt.NDArray, int, list[float]]:\n",
        "        V = np.zeros(3)\n",
        "        convergencehistory = []\n",
        "        for iteration in range(maxiter):\n",
        "            V_prev = V.copy()\n",
        "            for state_idx in range(3):\n",
        "                action = self.policyfunc[f'S{state_idx}']\n",
        "                expectedvalue = np.dot(\n",
        "                    self.transition_matrices[action][state_idx],\n",
        "                    V_prev)\n",
        "                V[state_idx] = self.rewards[f'S{state_idx}'] + discountfactor * expectedvalue\n",
        "            diff = np.max(np.abs(V - V_prev))\n",
        "            convergencehistory.append(diff)\n",
        "            if diff < 1e-8:\n",
        "                break\n",
        "        return V, iteration + 1, convergencehistory\n",
        "    def mdpsol(self, discountfactors: list[float]):\n",
        "        print(\"The MDP Problem Solution:\\n\")\n",
        "        for gamma in discountfactors:\n",
        "            print(f\"Discount Factor (γ): {gamma}\")\n",
        "            longtermrewards, iterations, convergence = self.calclongtermreward(gamma)\n",
        "            print(\"Long-Term Discounted Rewards:\")\n",
        "            for i, reward in enumerate(longtermrewards):\n",
        "                print(f\"State S{i}: {reward:.4f}\")\n",
        "            print(f\"Convergence Iterations: {iterations}\\n\")\n",
        "solver = mdpprog()\n",
        "discountfactors = [0.1, 0.01, 0.001, 0.3]\n",
        "solver.mdpsol(discountfactors)"
      ]
    }
  ]
}